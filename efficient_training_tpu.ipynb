{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"efficient_training_tpu_整理.ipynb","private_outputs":true,"provenance":[{"file_id":"1G8hcLw2XOfnvmHtxdXGAwBVcHYoxPgPs","timestamp":1624238523499}],"collapsed_sections":[]},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"widespread-checklist"},"source":["# 參考程式碼\n","# https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/"],"id":"widespread-checklist","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_tXY_Nj53zIW"},"source":["!pip install tensorflow_addons\n","!pip install focal_loss"],"id":"_tXY_Nj53zIW","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7EtCMx7tGxhZ"},"source":["# 因使用TPU模型必須需放在gcp storage上，這步驟需要給google colab存取gcp storage的權限\n","from google.colab import auth\n","import os\n","auth.authenticate_user()\n","\n","project_id = 'intrepid-vista-285204' # 需依照自己的project name命名\n","!gcloud config set project {project_id}"],"id":"7EtCMx7tGxhZ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oDB-HGD8owIi"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/T-Brain') #更改路徑"],"id":"oDB-HGD8owIi","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RwTTYnX6c4M5"},"source":[""],"id":"RwTTYnX6c4M5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"promotional-skill"},"source":["import os\n","import PIL\n","import PIL.Image\n","import pickle\n","import os\n","import re\n","\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.applications import EfficientNetB0\n","from tensorflow.keras.callbacks import LearningRateScheduler\n","from focal_loss import SparseCategoricalFocalLoss\n","from sklearn.utils import class_weight\n","\n","import autoaugment\n","tf.config.set_soft_device_placement(True)"],"id":"promotional-skill","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1AYjQGoU2gFi"},"source":["try:\n","    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n","    tf.config.experimental_connect_to_cluster(resolver)\n","    # This is the TPU initialization code that has to be at the beginning.\n","    tf.tpu.experimental.initialize_tpu_system(resolver)\n","    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n","    # strategy = tf.distribute.experimental.TPUStrategy(resolver)\n","    strategy = tf.distribute.TPUStrategy(resolver)\n","except ValueError:\n","    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n","    strategy = tf.distribute.MirroredStrategy()    "],"id":"1AYjQGoU2gFi","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6aS9WdPKYStZ"},"source":["TF records"],"id":"6aS9WdPKYStZ"},{"cell_type":"code","metadata":{"id":"HekihMv7VcSR"},"source":["# Load tf_records from gcp storage\n","\n","data_set_name = [\"test_cv2\", \"test_ori\"]\n","\n","label_info_path = [f'gs://esun--2021/tf_records/{name}/' for name in data_set_name]\n","train_path = [f'gs://esun--2021/tf_records/{name}/train/*' for name in data_set_name]\n","val_path = [f'gs://esun--2021/tf_records/{name}/val/*' for name in data_set_name]\n"],"id":"HekihMv7VcSR","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PLok4lAFaV5a"},"source":["print(label_info_path)\n","print(train_path)\n","print(val_path)"],"id":"PLok4lAFaV5a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-dgBh995VcVT"},"source":["# \n","def read_record(example):\n","    features = {\n","        \"image\": tf.io.FixedLenFeature([], tf.string),\n","        \"label\": tf.io.FixedLenFeature([], tf.int64),\n","    }\n","    example = tf.io.parse_single_example(example, features)\n","    \n","    image = tf.image.decode_jpeg(example[\"image\"], channels=3)\n","    image = tf.reshape(image, [224,224,3])\n","    label = tf.cast(example[\"label\"], tf.float32)\n","    \n","    return image, label"],"id":"-dgBh995VcVT","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jE34bSlsVcYl"},"source":["AUTO = tf.data.experimental.AUTOTUNE\n","\n","def prepare_dataset(file_path, order = False):\n","\n","    filenames = []\n","    for f_p in file_path:\n","        filenames.extend(tf.io.gfile.glob(f_p))\n","    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n","    \n","    # disregard the order of .tfrec files\n","    ignore_order = tf.data.Options()\n","    if order == False:\n","        ignore_order.experimental_deterministic = False\n","    else:\n","        ignore_order.experimental_deterministic = True\n","    dataset = dataset.with_options(ignore_order)\n","    \n","    dataset = dataset.map(read_record, num_parallel_calls=AUTO)\n","    return dataset\n","\n","\n","train_dataset = prepare_dataset(train_path)\n","val_dataset = prepare_dataset(val_path)"],"id":"jE34bSlsVcYl","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WyzUvr96YWyx"},"source":["\n","AUTO = tf.data.experimental.AUTOTUNE\n","# batch_size is scaled with the number of TPU cores\n","batch_size = 64 * strategy.num_replicas_in_sync\n","\n","def auto_aug(image, label):\n","    image = autoaugment.distort_image(image, aug_name='ra_aa', ra_num_layers=1, ra_magnitude=5)\n","    return image, label\n","\n","train_dataset = train_dataset.map(auto_aug, num_parallel_calls=AUTO)\n","train_dataset = train_dataset.shuffle(16384).repeat() \\\n","                .batch(batch_size, drop_remainder=True).prefetch(256)\n","\n","val_dataset = val_dataset.batch(batch_size, drop_remainder=True).prefetch(256)"],"id":"WyzUvr96YWyx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VkjoyX0feN7p"},"source":["# show training image\n","image, label = next(iter(train_dataset))\n","\n","fig, axes = plt.subplots(constrained_layout = True, nrows=3, ncols=3, figsize=(10, 10))\n","\n","for i in range(3):\n","    for j in range(3):\n","        axes[i][j].imshow(image[i*3+j], aspect=\"auto\")\n","        axes[i][j].axis(\"off\")"],"id":"VkjoyX0feN7p","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A51OEWOBpaeb"},"source":["# show val image\n","image, label = next(iter(val_dataset))\n","\n","fig, axes = plt.subplots(constrained_layout = True, nrows=3, ncols=3, figsize=(10, 10))\n","\n","for i in range(3):\n","    for j in range(3):\n","        axes[i][j].imshow(image[i*3+j], aspect=\"auto\")\n","        axes[i][j].axis(\"off\")"],"id":"A51OEWOBpaeb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d7bZTroyyKvW"},"source":[""],"id":"d7bZTroyyKvW","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wjTQbrGrc0Zi"},"source":["with strategy.scope():\n","\n","    inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n","    efficient_model = EfficientNetB0(include_top=True, weights=None, classes=801, input_tensor=inputs)\n","    model = tf.keras.Model(inputs, efficient_model.outputs)\n","    \n","    model.compile(optimizer=tf.keras.optimizers.Adam(),\n","                 loss=SparseCategoricalFocalLoss(gamma=2),\n","                 metrics=[\"sparse_categorical_accuracy\"])  \n","\n","    # model.load_weights('./effv1_model_0618_final_v1_2.hdf5')"],"id":"wjTQbrGrc0Zi","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0RNtHGBPyCIs"},"source":["model.summary()"],"id":"0RNtHGBPyCIs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aEcp9Mqa4J5z"},"source":["# Learning rate scheduler\n","def decay(inp):\n","    \n","    lr_init = 0.00005\n","    # max learning rate is scaled with the number of TPU cores\n","    lr_max = 0.000125 * strategy.num_replicas_in_sync\n","    lin_lr = 5\n","    if inp <= lin_lr:\n","        lr = inp*(lr_max - lr_init) / lin_lr + lr_init\n","    else:\n","        lr = lr_max * np.exp(-0.1*(inp - lin_lr))\n","    lr *= 0.1\n","    return lr\n","\n","lrs = LearningRateScheduler(decay)"],"id":"aEcp9Mqa4J5z","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LjVhNPfuuGVb"},"source":["import json\n","import os\n","label_cnt_dict = {}\n","for file_path in label_info_path:\n","  d = tf.io.read_file(filename=os.path.join(file_path, 'label_cnt_dict.json'))\n","\n","  tmp_dict = json.loads(str(d.numpy())[2:-1])\n","  print(file_path)\n","  print(len(tmp_dict))\n","  for k, v in tmp_dict.items():\n","    k = int(k)\n","    if k not in label_cnt_dict:\n","      label_cnt_dict[k] = v\n","    else:\n","      label_cnt_dict[k] += v\n","print(label_cnt_dict)"],"id":"LjVhNPfuuGVb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4cFJh-UbuQw3"},"source":["label_cnt_lst = []\n","for k, v in label_cnt_dict.items():\n","  label_cnt_lst.extend([k]*v)\n","class_weights = class_weight.compute_class_weight(\n","          'balanced',\n","          np.unique(label_cnt_lst), \n","          label_cnt_lst)\n"],"id":"4cFJh-UbuQw3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W9fw52kmu01M"},"source":["class_weight = {idx:weight for idx, weight in enumerate(class_weights)}"],"id":"W9fw52kmu01M","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NVmawj0Kqkbc"},"source":["print(len(label_cnt_lst))"],"id":"NVmawj0Kqkbc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zuisrdimvNgg"},"source":["len(class_weight)"],"id":"zuisrdimvNgg","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OvxrpGtd8EA-"},"source":["\n","early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto', restore_best_weights=True)\n","ckpt = tf.keras.callbacks.ModelCheckpoint('./service/fine_tuned_model/efficient_v1.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n","\n","history = model.fit(train_dataset,\n","            validation_data=val_dataset,\n","            steps_per_epoch=len(label_cnt_lst)//batch_size,\n","            class_weight=class_weight,\n","            epochs=10,\n","            callbacks=[lrs, ckpt, early_stop])"],"id":"OvxrpGtd8EA-","execution_count":null,"outputs":[]}]}