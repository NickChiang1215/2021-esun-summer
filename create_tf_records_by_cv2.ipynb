{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf.records\n",
    "# https://www.kaggle.com/ryanholbrook/walkthrough-building-a-dataset-of-tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Example, Features, Feature\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './test_img/'\n",
    "target_dir = './tf_records/test'\n",
    "\n",
    "cv2_target_dir = target_dir+'_cv2'\n",
    "ori_target_dir = target_dir+'_ori'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('esun_ocr_target.txt', 'r') as f:\n",
    "    d = list(f.read())\n",
    "d.append('isnull')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_File(file_dir, shuffle=True):\n",
    "    # The images in each subfolder\n",
    "    images = []\n",
    "    # The subfolders\n",
    "    subfolders = []\n",
    "\n",
    "    # Using \"os.walk\" function to grab all the files in each folder\n",
    "    for dirPath, dirNames, fileNames in os.walk(file_dir):\n",
    "        for name in fileNames:\n",
    "            images.append(os.path.join(dirPath, name))\n",
    "\n",
    "        for name in dirNames:\n",
    "            subfolders.append(os.path.join(dirPath, name))\n",
    "\n",
    "    # To record the labels of the image dataset\n",
    "    labels = []\n",
    "    label_2_id = {}\n",
    "    for id_, a_folder in enumerate(subfolders):\n",
    "        \n",
    "        n_img = len(os.listdir(a_folder))\n",
    "        label_char = a_folder.split('/')[-1]\n",
    "        label_2_id[label_char] = id_\n",
    "        labels = np.append(labels, n_img * [id_])\n",
    "\n",
    "    labels = labels.astype(int)\n",
    "    \n",
    "    combine_lst = list(zip(images, labels))\n",
    "    if shuffle:\n",
    "        random.shuffle(combine_lst)\n",
    "    images, labels = zip(*combine_lst)\n",
    "\n",
    "    return images, labels, label_2_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labs, label_2_id = get_File(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "idx2label = {idx:label for label, idx in label_2_id.items()}\n",
    "\n",
    "for target_dir in [cv2_target_dir, ori_target_dir]:\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.mkdir(target_dir)\n",
    "    path = os.path.join(target_dir, 'label_map.json')\n",
    "    with open(path, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(label_2_id, outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx2label = {idx:label for label, idx in label_2_id.items()}\n",
    "\n",
    "for target_dir in [cv2_target_dir, ori_target_dir]:\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.mkdir(target_dir)\n",
    "    path = os.path.join(target_dir, 'label_map.json')\n",
    "    with open(path, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(label_2_id, outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(imgs, labs, test_size=0.2, random_state=123, shuffle=True, stratify=labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(\n",
    "          'balanced',\n",
    "          np.unique(y_train), \n",
    "          y_train)\n",
    "\n",
    "for target_dir in [cv2_target_dir, ori_target_dir]:\n",
    "    path = os.path.join(target_dir, 'class_weight.json')\n",
    "    with open(path, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(list(class_weights), outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cnt_dict = {}\n",
    "for i in label_2_id.values():\n",
    "    label_cnt_dict[str(i)] = y_train.count(i)\n",
    "    \n",
    "for target_dir in [cv2_target_dir, ori_target_dir]:\n",
    "    path = os.path.join(target_dir, 'label_cnt_dict.json')\n",
    "    with open(path, 'w', encoding='utf-8') as fp:\n",
    "        json.dump(label_cnt_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example(encoded_image, label):\n",
    "    image_feature = Feature(\n",
    "        bytes_list=BytesList(value=[\n",
    "            encoded_image,\n",
    "        ]),\n",
    "    )\n",
    "    label_feature = Feature(\n",
    "        int64_list=Int64List(value=[\n",
    "            label,\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    features = Features(feature={\n",
    "        'image': image_feature,\n",
    "        'label': label_feature,\n",
    "    })\n",
    "    \n",
    "    example = Example(features=features)\n",
    "    \n",
    "    return example.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tf_records_qty = 32\n",
    "val_tf_records_qty = 8\n",
    "\n",
    "kernel_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "qty_per_tf_records = math.ceil(len(X_train) / training_tf_records_qty)\n",
    "\n",
    "NUM_SHARDS = 32\n",
    "\n",
    "for target_dir in [cv2_target_dir, ori_target_dir]:\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.mkdir(target_dir)\n",
    "\n",
    "    if not os.path.exists(os.path.join(target_dir, 'train')):\n",
    "        os.mkdir(os.path.join(target_dir, 'train'))\n",
    "    \n",
    "\n",
    "PATH_cv2 = os.path.join(os.path.join(cv2_target_dir, 'train'), 'shard_train_{:02d}.tfrecord')\n",
    "PATH_ori = os.path.join(os.path.join(ori_target_dir, 'train'), 'shard_train_{:02d}.tfrecord')\n",
    "\n",
    "total_training_examples = len(X_train)\n",
    "cnt = 0\n",
    "for shard in range(training_tf_records_qty):\n",
    "    with tf.io.TFRecordWriter(path=PATH_cv2.format(shard)) as f_cv2, tf.io.TFRecordWriter(path=PATH_ori.format(shard)) as f_ori: # , open('b', 'w') as b\n",
    "        for idx, (img_path, lab) in enumerate(zip(X_train, y_train)):\n",
    "            img=cv2.imdecode(np.fromfile(img_path,dtype=np.uint8),1)\n",
    "            img = img[:,:,::-1]\n",
    "            image = img\n",
    "            img = cv2.GaussianBlur(img,(kernel_size, kernel_size), 0)\n",
    "            img = cv2.Canny(img, 150, 200)\n",
    "            img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "\n",
    "            # 這段似乎有點智障!? 想一下怎麼改\n",
    "            img = tf.convert_to_tensor(img)\n",
    "            img = tf.image.convert_image_dtype(img, dtype=tf.uint8)\n",
    "            img = tf.io.encode_jpeg(img).numpy()\n",
    "\n",
    "            f_cv2.write(make_example(img, lab))\n",
    "\n",
    "            image = cv2.resize(image, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "            image = tf.image.convert_image_dtype(image, dtype=tf.uint8)\n",
    "            image = tf.io.encode_jpeg(image).numpy()              \n",
    "\n",
    "            f_ori.write(make_example(image, lab))\n",
    "            cnt += 1\n",
    "            if idx >= qty_per_tf_records-1:\n",
    "                X_train = X_train[qty_per_tf_records:]\n",
    "                y_train = y_train[qty_per_tf_records:]             \n",
    "                break\n",
    "                \n",
    "assert cnt == total_training_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "qty_per_tf_records = math.ceil(len(X_test) / val_tf_records_qty)\n",
    "\n",
    "for target_dir in [cv2_target_dir, ori_target_dir]:\n",
    "    if not os.path.exists(os.path.join(target_dir, 'val')):\n",
    "        os.mkdir(os.path.join(target_dir, 'val'))\n",
    "\n",
    "PATH_cv2 = os.path.join(os.path.join(cv2_target_dir, 'val'), 'shard_val_{:02d}.tfrecord')\n",
    "PATH_ori = os.path.join(os.path.join(ori_target_dir, 'val'), 'shard_val_{:02d}.tfrecord')\n",
    "\n",
    "total_val_examples = len(X_test)\n",
    "cnt = 0\n",
    "for shard in range(val_tf_records_qty):\n",
    "    with tf.io.TFRecordWriter(path=PATH_cv2.format(shard)) as f_cv2, tf.io.TFRecordWriter(path=PATH_ori.format(shard)) as f_ori:\n",
    "        for idx, (img_path, lab) in enumerate(zip(X_test, y_test)):\n",
    "            img=cv2.imdecode(np.fromfile(img_path,dtype=np.uint8),1)\n",
    "            img = img[:,:,::-1]\n",
    "\n",
    "            image = img\n",
    "            \n",
    "            img = cv2.GaussianBlur(img,(kernel_size, kernel_size), 0)\n",
    "            img = cv2.Canny(img, 150, 200)\n",
    "            img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            \n",
    "            # 這段似乎有點智障!? 想一下怎麼改\n",
    "            img = tf.convert_to_tensor(img)\n",
    "            img = tf.image.convert_image_dtype(img, dtype=tf.uint8)\n",
    "            img = tf.io.encode_jpeg(img).numpy()\n",
    "\n",
    "            f_cv2.write(make_example(img, lab))\n",
    "            \n",
    "            image = cv2.resize(image, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "            image = tf.image.convert_image_dtype(image, dtype=tf.uint8)\n",
    "            image = tf.io.encode_jpeg(image).numpy()              \n",
    "\n",
    "            f_ori.write(make_example(image, lab))            \n",
    "            \n",
    "            cnt += 1\n",
    "            if idx >= qty_per_tf_records-1:\n",
    "                X_test = X_test[qty_per_tf_records:]\n",
    "                y_test = y_test[qty_per_tf_records:]                \n",
    "                break\n",
    "                \n",
    "assert cnt == total_val_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-botswana",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
